\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage[margin =1.0in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{biblatex}
\addbibresource{ref.bib}

%\graphicspath{{report_figures/}}
\title{Computational Techniques in Latent Variable Network Models}
\author{Lily Chou, Ben Draves, Nathan Josephs, Kelly Kung}
\date{December 12, 2018}

\begin{document}

\maketitle

\section{Introduction}

\textit{Game of Thrones} is a popular HBO TV series adapted from George R.R. Martin's best-selling book series \textit{A Song of Ice and Fire}. The medieval fantasy epic describes the stories of powerful families - kings and queens, knights and renegades, liars and honest men - playing a deadly game for control of the Seven Kingdoms of Westeros and to sit atop the Iron Throne. Conspiracy and deception, power and exile, blood and tears run through the plot, sewing together characters with various backgrounds including royals and peasants, as well as ice zombies and dragons. As the plot develops with each book release, readers wonder where the storyline leads. Within the Seven Kingdoms, enemies become friends and vice-versa, all the while winter spreads as the battle of ice and far draws nearer. For many characters, it is clear who is good and who is evil, but such moral assignments are skewed by the readers' biases. Here, we propose to put aside our personal feelings and let the data decide.

After discovering a dataset on the exchanges between the characters from the third book, \textit{A Storm of Swords}, we start to wonder if, and what, information we can extract. In particular, how can we make inferences on character being good or bad. To address this, we turn to research of Peter Hoff on latent network models. In Section \ref{Data}, we show how the relationships between characters from the book naturally arise as a network. Before fitting a model, we explain the latent network model framework in Section \ref{Models}. We then present two methods for fitting our model in Section \ref{Computational Methods} and then compare the results of the two methods in Section \ref{Results}. Finally, in Section \ref{Conclusion} we end with a discussion on the implications of our findings, as well as possible future work. We provide our code in the Appendix.

\section{Data}\label{Data}
Due to its global fame, \textit{Game of Thrones} has been studied in many different contexts, especially in network analysis. Therefore, there are many readily available datasets. In our project, we use the dataset from \cite{beveridge2016network}, which contains information about characters' interactions in the third book of the series. In this case, an \textit{interaction} occurs if the characters' names appear within fifteen words of one another. This could mean that the characters interacted with each other, conversed with each other, or were generally mentioned together by another means. There is also a column that contains the number of times each pair interacts with one another. Using this dataset, we constructed a weighted network using the number of interactions as weights. Here, the nodes represent the characters and the edges represent the interactions. We use an adjacency matrix, $A$, to represent the network, where the $a_{i,j}$ element represents the number of times the characters interacted with each other. Note that this means if $a_{i.j} = 0$, there are no recorded interactions between character $i$ and $j$ based on how an \textit{interaction} is defined. Although the original dataset is intended as a directed network, we treat it as an undirected network in order to simplify our models.

After transforming the dataset, our network $G$ contains $N_V(G) = 107$ nodes and $N_E(G) = 352$ edges which means it is quite sparse since it only contains approximately $6.20\%$ of ${N_V(G) \choose 2} = 5,671$ possible edges. Figure \textbf{include figure} shows the network described. In order to account for the sparsity of our network, we consider a subnetwork which only contains pairs of characters with at least 75 interactions \textbf{or more?}. We chose a cutoff of 75 interactions because we want to focus our analysis on only the main characters. Looking at the distribution of the weighted degree, we see that $65.42\%$ of the characters had fewer than 75 interactions. Therefore, it makes sense to use this cutoff to limit our analysis to only the main characters. By doing so, our new network $G'$ contains $N_V(G') = 35$ nodes and $N_E(G') = 140$ edges. Here, we see that the network now contains $23.53\%$ of 595 possible edges, which is a more appropriate level of density for our analysis. Everything that follows is done on this subnetwork $G'$. Figure \textbf{include figure next to other figure} shows this subnetwork $G'$, and indeed, we recognize the main characters remain in our network.

\section{Models} \label{Models}

\subsection{Latent Network Models}\label{LNM}
Review of Hoff's papers (need's citations):
\begin{enumerate}
    \item Introduction of network data
    \item Community detection in networks
    \item Latent variable models 
    \item Inclusion of observed covariates
\end{enumerate}

\subsection{Model Formulation}\label{Model Formulation}

Following the layout in Section \ref{LNM}, we model the presence of an edge given our latent variables as
\[\text{logit} \mathbb{P}(Y_{ij} = 1|Z) = ||Z_i - Z_j|| + \epsilon_{ij}\]
where
\[Z_i \overset{iid}{\sim}\sum_{g=1}^G \lambda_g\text{MVN}_d(\mu_g,\sigma_g^2I_d)\]
Putting priors over $\mu_g$ and  $\sigma_g^2$, as well as introducing the latent variable $K$ representing the group from which $Z$ is drawn, we have the following model formulation:

\begin{align*}
Y_{ij} | Z_i, Z_j &\sim \text{Bern}\Big[\text{logit}^{-1}\big(\Vert Z_i - Z_j \Vert\big)\Big] \\
Z_i | K_i = k_i &\overset{iid}\sim N(\mu_{k_i}, \sigma_{k_i}^2 I_2) \\
K &\overset{iid}\sim \text{Multinoulli}\big(G, \lambda_g\big) \\
\lambda_g &\overset{iid}\sim \frac{1}{G} \\
\mu_{k_i} &\overset{iid}\sim \text{MVN}_2(0, I_2) \\
\sigma_{k_i}^2 &\overset{iid}\sim \text{Inv}\Gamma(1,1)
\end{align*}
With these, we can write the complete likelihood and log-likelihood, which we will use in both our fitting procedures. We let $\theta = (\mu, \sigma^2, K)$ denote our nuisance parameters. Then, we have

\begin{align*}
\mathcal{L}(Z, \theta; Y) &= \prod_{i<j}\mathbb{P}(Y_{ij} | Z_i, Z_j) \mathbb{P}(Z_i | K_i, \mu_{k_i}, \sigma_{k_i}^2) \mathbb{P}(Z_j | K_j, \mu_{k_j}, \sigma_j^2) \\ 
&\ \ \ \ \ \ \ \ \times  \mathbb{P}(K_i | \lambda_i) \mathbb{P}(\lambda_i)\mathbb{P}(\mu_{k_i})\mathbb{P}(\sigma_{k_i}^2)\mathbb{P}(K_j)\mathbb{P}(\mu_{k_j})\mathbb{P}(\sigma_{k_j}^2) \\
&= \prod_{i<j}\Big(\text{logit}^{-1}\big(\Vert Z_i-Z_j\Vert)\Big)^{Y_{ij}}\Big(1 - \text{logit}^{-1}\big(\Vert Z_i-Z_j\Vert)\Big)^{1 - Y_{ij}} \\
&\ \ \ \ \ \ \ \ \times f_{MVN}(\mu_{k_i}, \sigma_{k_i}^2I_2)\times\lambda_i \times \frac{1}{G} \times f_{MVN}(0, I_2) \times f_{\text{Inv}\chi^2_2} \\
&\ \ \ \ \ \ \ \ \times f_{MVN}(\mu_{k_j}, \sigma_{k_j}^2I_2)\times  \lambda_j \times \frac{1}{G} \times f_{MVN}(0, I_2) \times f_{\text{Inv}\chi^2_2} \\
&\propto \prod_{i<j}\Big(\text{logit}^{-1}\big(\Vert Z_i-Z_j\Vert)\Big)^{Y_{ij}}\Big(1 - \text{logit}^{-1}\big(\Vert Z_i-Z_j\Vert)\Big)^{1 - Y_{ij}} \\
&\ \ \ \ \ \ \ \ \times \frac{1}{\sigma_{k_i}}\exp\Big\{-\frac{1}{2\sigma_{k_i}^2}(Z_i - \mu_{k_i})^T( Z_i - \mu_{k_i}) \Big\} \enspace \frac{1}{\sigma_{k_j}^2}\exp\Big\{-\frac{1}{2\sigma_{k_j}}(Z_j - \mu_{k_j})^T (Z_j - \mu_{k_j})\Big\} \\
&\ \ \ \ \ \ \ \ \times \exp\Big\{-\frac{1}{2}\mu_{k_i}^T\mu_{k_i}\Big\}\exp\Big\{-\frac{1}{2}\mu_{k_j}^T\mu_{k_j}\Big\}  \times \frac{1}{(\sigma_{k_i}^2)^2}\exp\Big\{-\frac{1}{\sigma_{k_i}^2}\Big\}\frac{1}{(\sigma_{k_j}^2)^2}\exp\Big\{-\frac{1}{\sigma_{k_j}^2}\Big\} \times \lambda_i \times \lambda_j
\end{align*}

Next, we turn to finding the full conditional of each parameter in this likelihood function. At each iteration of the Gibbs sampler, $t$, we conditional on the current parameter vector $\theta^{(t)}$ and sample each parameter in stepwise fashion. We write the full conditionals as follows. 

\begin{align*}
f_{\mu_{k_i}|\theta^{(t)}, Y}(\mu_{k_i}|\theta^{(t)},Y) &\propto \exp\left\{-\frac{1}{2\sigma_{k_i}^2}(Z_i - \mu_{k_i})^T(Z_i - \mu_{k_i})\right\}\exp\left\{-\frac{1}{2}\mu_{k_i}^T\mu_{k_i}\right\}\\
&\propto \exp\left\{-\frac{1}{2\sigma_{k_i}^2}(Z_i^TZ_i - 2Z_i^T\mu_{k_i} + \mu_{k_i}^T\mu_{k_i}) - \frac{1}{2}\mu_{k_i}^T\mu_{k_i}\right\}\\
&\propto\exp\left\{-\frac{Z_i^TZ_i}{2\sigma_{k_i}^t} + \frac{Z_i^T\mu_{k_i}}{\sigma_{k_i}^2} - \left(\frac{1}{2\sigma_{k_i}^2} + \frac{1}{\sigma_{k_i}^2}\right)\mu_{k_i}^T\mu_{k_i}\right\}\\
&\propto\exp\left\{-\frac{(\sigma_{k_i}^2+1)}{2\sigma_{k_i}^t}\left(\mu_{k_i}-\frac{Z_i}{(\sigma_{k_i}^2+1)}\right)^T\left(\mu_{k_i}-\frac{Z_i}{(\sigma_{k_i}^2+1)}\right)\right\}\\
\end{align*}
We can complete an identical analysis for $\mu_{k_j}$ and we arrive at the following distributions. 
\begin{align*}
\mu_{k_i}|\theta^{(t)},Y&\sim f_{MVN}\left(\frac{Z_i^{(t)}}{(\sigma_{k_i}^2)^{(t)}+1}, \frac{(\sigma_{k_i}^2)^{(t)}}{(\sigma_{k_i}^2)^{(t)}+1}I_2\right)\\
\mu_{k_j}|\theta^{(t)},Y&\sim f_{MVN}\left(\frac{Z_j^{(t)}}{(\sigma_{k_j}^2)^{(t)}+1}, \frac{(\sigma_{k_j}^2)^{(t)}}{(\sigma_{k_j}^2)^{(t)}+1}I_2\right)
\end{align*}

Now, we turn to finding the full conditional for the latent variance parameters $\sigma_{k_i}^2$. 

\begin{align*}
f_{\sigma_{k_i}^2|\theta, Y}(\sigma_{k_i}^2|\theta, Y) &\propto \frac{1}{2\sigma_{k_i}}\exp\left\{-\frac{1}{2\sigma_{k_i}^2}(Z_i-\mu_{k_i})^T(Z_i-\mu_{k_i})\right\}\frac{1}{(\sigma_{k_i}^2)^2}\exp\left\{-\frac{1}{\sigma_{k_i}^2}\right\}\\
&\propto (\sigma_{k_i}^2)^{-3/2-1}\exp\left\{-\frac{1}{\sigma_{k_i}^2}\left(\frac{(Z_i-\mu_{k_i})^T(Z_i-\mu_{k_i})}{2}+1\right)\right\}
\end{align*}

We can complete an identical analysis for $\sigma^2_{k_j}$ and we arrive at the following distributions. 

\begin{align*}
\sigma_{k_i}^2|\theta^{(t)}, Y &\sim \text{Inv}\Gamma\left(3/2, \frac{(Z_i - \mu_{k_i})^T(Z_i - \mu_{k_i})}{2} + 1\right)\\
\sigma_{k_j}^2|\theta^{(t)}, Y &\sim \text{Inv}\Gamma\left(3/2, \frac{(Z_j - \mu_{k_i})^T(Z_j - \mu_{k_j})}{2} + 1\right)\\
\end{align*}


%Hence the log-likelihood, up to a constant, is
%\begin{align*}l(Z, \theta ; Y) &\overset{c}= \sum_{i<j} \Big\{Y_{ij} \Vert Z_i - Z_j \Vert - \log \big(1 + \exp\big(\Vert Z_i - Z_j\Vert\big)\big)\Big\} \\&\ \ \ \ \ \ \ \ - \log \sigma_{k_i} - \frac{1}{2\sigma_{k_i}^2}(Z_i - \mu_{k_i})^T( Z_i - \mu_{k_i}) - \log \sigma_{k_j} - \frac{1}{2\sigma_{k_j}^2}(Z_j - \mu_{k_j})^T (Z_j - \mu_{k_j})  \\&\ \ \ \ \ \ \ \ -\frac{1}{2}\Big(\mu_{k_i}^T\mu_{k_i} + \mu_{k_j}^T\mu_{k_j} + \sigma_{k_i}^2 + \sigma_{k_j}^2\Big)  + \log \lambda_i + \log \lambda_j\end{align*}


%&\ \ \ \ \ \ \ \ - \frac{1}{2}\log 2\pi - \frac{1}{2}\log \sigma_k^2 - \frac{(Z_i - \mu_k)^2}{2\sigma_k^2}  - \frac{1}{2}\log 2\pi - \frac{1}{2}\log \sigma_j^2 - \frac{(Z_j - \mu_j)^2}{2\sigma_j^2} \\
%&\ \ \ \ \ \ \ \ - \log G - \log G \\
%&\ \ \ \ \ \ \ \ - \frac{1}{2} \log \big(2\pi\det(I_2)\big) - \frac{1}{2}(\mu_k^TI_2^{-1}\mu_k) - \frac{1}{2} \log \big(2\pi\det(I_2)\big) - \frac{1}{2}(\mu_j^TI_2^{-1}\mu_j) \\
%&\ \ \ \ \ \ \ \ - \frac{1}{2}\log 2 - \log \Gamma(\frac{1}{2}) - \frac{1}{2}\log \sigma_k^2 - \frac{\sigma_k^2}{2} - \frac{1}{2}\log 2 - \log \Gamma(\frac{1}{2}) - \frac{1}{2}\log \sigma_j^2 - \frac{\sigma_j^2}{2} \\
%&\overset{c}= \text{simplified up to constant}

%Below we write $\mathcal{L}_1$, $\ell_1$ are likelihood methods for the unweighted network model. If we wish to include the weights into our modeling procedure, we construct a second log-likelihood $\ell_2$. 
%\begin{align}
%\mathcal{L}_1(\mathbf{Z}; \mathbf{Y}) &= \prod_{i<j} \left\{\text{logit}^{-1}(||Z_i - Z_j||)\right\}^{Y_{ij}}\left\{1-\text{logit}^{-1}(||Z_i - Z_j||)\right\}^{1-Y_{ij}}\\
%\ell_1(\mathbf{Z};\mathbf{Y}) &=  \sum_{i<j}Y_{ij}||Z_i - Z_j|| - \log\left(1 + \exp\left(||Z_i - Z_j||\right)\right)\\ 
%\ell_2(\mathbf{Z}; \mathbf{Y}) &= \sum_{i<j}y_{ij}(||Z_i - Z_j||) - \exp(||Z_i - Z_j||) - \log(y_{ij})\\
%\end{align}

\section{Computational Methods} \label{Computational Methods}

\subsection{EM}

One method for finding the latent variables $Z_i$ for each node is the Expectation-Maximization (EM) algorithm. Unfortunately, the log-likelihood given in Section \ref{Model Formulation} cannot easily be handled with EM. In particular, finding $\mathbb{E}_{Z_i, Z_j | Y, \theta}\Big[l(Z, \theta ; Y)\Big]$ is difficult. One solution is to sample and use Monte Carlo estimation in the E-step, then proceed with the M-step. This is the so-called Monte Carlo EM (MCEM) method.

Instead, we simplify our model to make EM more analytically tractable. In Section \ref{MCMC}, we fit the full model using MCMC. Here, we fit the simplified model:

\begin{align*}
Y_{ij} | d_{ij} &\sim \text{Bern}(d_{ij}) \\
d_{ij} &\sim \text{Beta}(\alpha, \beta)
\end{align*}
where $d_{ij} \equiv \text{logit}^{-1}\Vert Z_i - Z_j \Vert$ are latent (standardized) distances. Then

\begin{align*}
\mathcal{L}(d, \alpha, \beta; Y) &= \prod_{i<j}d_{ij}^{Y_{ij}}\big(1 - d_{ij}\big)^{1 - Y_{ij}} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}d_{ij}^{\alpha - 1}\big(1-d_{ij}\big)^{\beta - 1} \\
l(d, \alpha, \beta ; Y) &= \sum_{i<j} Y_{ij}\log \Big(\frac{d_{ij}}{1 - d_{ij}}\Big) + (1-d_{ij})  \\
&\ \ \ \ \ \ \ \ + \log \Gamma(\alpha + \beta) - \log \Gamma(\alpha) - \log \Gamma(\beta) \\
&\ \ \ \ \ \ \ \ + (\alpha - 1)\log d_{ij} + (\beta - 1)\log(1 - d_{ij})
\end{align*}

%Gamma
%\begin{align*}
%\mathcal{L}(d_{ij}, \alpha, \beta; Y) &= \prod_{i<j}\Big(\text{logit}^{-1}\big(d_{ij})\Big)^{Y_{ij}}\Big(1 - \text{logit}^{-1}\big(d_{ij})\Big)^{1 - Y_{ij}} \\
%&\ \ \ \ \ \ \ \ \times \frac{\beta^\alpha}{\Gamma(\alpha)}d_{ij}^{\alpha - 1}\exp\big\{-\beta d_{ij}\big\} \\
%l(d_{ij}, \alpha, \beta ; Y) &= \sum_{i<j} Y_{ij}d_{ij} - \log \big(1 + \exp\big(d_{ij}\big)\big) \\
%&\ \ \ \ \ \ \ \ + \alpha \log \beta - \log \Gamma(\alpha) + (\alpha - 1) \log d_{ij} - \beta d_{ij}
%\end{align*}

\subsubsection{E-Step}

For the E-step, we observe that the beta distribution is conjugate to the binomial. Therefore, we have

\begin{align*}
d_{ij} | Y_{ij}, \theta &\propto \text{Bern}(d_{ij})\text{Beta}(\alpha, \beta)\\
&= \text{Beta}\Big(\alpha + Y_{ij}, \beta + 1 - Y_{ij}\Big)
\end{align*}
Therefore, we have
\begin{align*}
\mathbb{E}_{d_{ij} | Y_{ij}, \theta} [d_{ij}] &= \frac{\alpha + Y_{ij}}{\alpha + \beta + 1} \\
\mathbb{E}_{d_{ij} | Y_{ij}, \theta} \big[\log d_{ij}\big] &= \Psi\Big(\alpha + Y_{ij}\Big) - \Psi\Big(\alpha + \beta + 1\Big) \\
\mathbb{E}_{d_{ij} | Y_{ij}, \theta} \big[\log (1 - d_{ij})\big] &= \Psi\Big(\beta + 1 - Y_{ij}\Big) - \Psi\Big(\alpha + \beta + 1\Big)
\end{align*}
where $\Psi$ is the digamma function. Thus
\begin{align*}
Q\big(\theta; \theta^{(t)}\big) &\equiv \mathbb{E}_{d | Y, \theta^{(t)}} \Big[l(d; Y, \theta)\Big] \\
&= \sum_{i<j} (Y_{ij} + \alpha - 1) \mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} [\log  d_{ij}] - (Y_{ij} - \beta + 1) \mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} \big[\log (1-d_{ij})\big] \\
&\ \ \ \ \ \ \ \ - \mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} [d_{ij}] + 1 + \log \Gamma(\alpha + \beta) - \log \Gamma(\alpha) - \log \Gamma(\beta) \tag{E}
\end{align*}

\subsubsection{M-Step}

For the M-step, we simply take the partial derivatives of $Q\big(\theta; \theta^{(t)}\big)$ with respect to $\alpha$ and $\beta$. Note that we will need an approximate solution in both cases since the digamma function prevents us from finding an analytic solution. For this, we use the Newton-Raphson Method.

\begin{align*}
\frac{\partial Q\big(\theta; \theta^{(t)}\big)}{\partial \alpha^{(t)}} &= \sum_{i<j} \mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} \big[\log d_{ij}\big] + \Psi\Big(\alpha^{(t+1)} + \beta^{(t)}\Big) - \Psi(\alpha^{(t+1)})= 0 \\
&\implies \Psi\Big(\alpha^{(t+1)} + \beta^{(t)}\Big) - \Psi(\alpha^{(t+1)}) = -\frac{\sum_{i<n}\mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} \big[\log d_{ij}\big]}{{n \choose 2}} \tag{M1} \\
\frac{\partial Q\big(\theta; \theta^{(t)}\big)}{\partial \beta^{(t)}} &= \sum_{i<j} \mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} \big[\log (1- d_{ij})\big] + \Psi\Big(\alpha^{(t+1)} + \beta^{(t+1)}\Big) - \Psi(\beta^{(t+1)}) = 0 \\
&\implies \Psi\Big(\alpha^{(t+1)} + \beta^{(t+1)}\Big) - \Psi(\beta^{(t+1)}) = -\frac{\sum_{i<n}\mathbb{E}_{d_{ij} | Y_{ij}, \theta^{(t)}} \big[\log (1 - d_{ij})\big]}{{n \choose 2}} \tag{M2} \\
\frac{\partial^2 Q\big(\theta; \theta^{(t)}\big)}{\partial {\alpha^{(t)}}^2} &= \Psi'\Big(\alpha^{(t+1)} + \beta^{(t)}\Big) - \Psi'(\alpha^{(t+1)}) \\
\frac{\partial^2 Q\big(\theta; \theta^{(t)}\big)}{\partial {\beta^{(t)}}^2} &= \Psi'\Big(\alpha^{(t+1)} + \beta^{(t)}\Big) - \Psi'(\beta^{(t+1)}) \\
\frac{\partial^2 Q\big(\theta; \theta^{(t)}\big)}{\partial {\alpha^{(t)}}\beta^{(t)}} &= \Psi'\Big(\alpha^{(t+1)} + \beta^{(t)}\Big)
\end{align*}
Question about time superscripts.
\subsubsection{Pseudocode}

\begin{algorithm*}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \underline{LNM EM} $(G)$\;
    \Input{Graph $G$}
    \Output{Latent Distances $d_{ij}$}
    E\;
    M\;
    \caption{EM for simplified latent network model}
\end{algorithm*}

\subsection{MCMC}\label{MCMC}

\begin{algorithm*}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \underline{LNM MCMC} $(G, \textbf{params}, ns)$\;
    \Input{Graph $G$\\
    \textbf{params}\\
    Number of samples $ns$}
    \Output{Posterior p(\textbf{WHAT})}
    Initialize \textbf{what}\;
    \For{$t = 2,\ldots,ns$}
      {
      \textbf{sampler}
      }
    \caption{Gibbs sampler for latent network model}
\end{algorithm*}

\section{Results} \label{Results}
\begin{enumerate}
    \item Analysis
    \item Results
\end{enumerate}

\subsection{EM}

\subsection{MCMC}

\subsection{Comparison}

\section{Conclusion} \label{Conclusion}

book 3 is nice because its midpoint of releases so characters have developed, but what if we perform same analysis for the other books and map a character's goodness across each book. Also, could apply MCEM, or for simplified latent distance model, we could choose any GLM and change to conjugate prior

\section{References}
\printbibliography

\section*{Appendix}
\appendix 

\section{Estimation Maximization Code}
\section{Markov Chain Monte Carlo Code}
\section{Figures Code}

\end{document}
